services:
  # this is the comfy service, any kind of existing comfy container could be used
  comfyui:
    image: ghcr.io/lonelyowl13/comfy_simple:latest
    volumes:
      - ./models/:/ComfyUI/models/
      - ./custom_nodes/:/ComfyUI/custom_nodes/
      - ./output:/ComfyUI/output
      - ./input:/ComfyUI/input
      - ./my_workflows:/ComfyUI/my_workflows
      - ./ComfyUI_temp:/ComfyUI_temp
      - ./comfyui.log:/ComfyUI/comfyui.log

    ports:
      - "8188:8188"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
              count: all

  # this is an api-wrapper service, it runs in a separate container
  api:
    image: comfy-api
    volumes:
      - ./models:/models
      - ./output:/output
      - ./input:/inputs
      - ./example-workflows:/workflows
    depends_on:
      comfyui:
        condition: service_started
    build:
      context: . 
    ports:
    - "3000:3000"
    environment:
      - DIRECT_ADDRESS=comfyui
    command: ["./comfyui-api"]
